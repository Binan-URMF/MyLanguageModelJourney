{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "# Read the CSV file\n",
    "folder = 'inaugural/'\n",
    "# Get a list of all text files in the folder\n",
    "text_files = glob.glob(os.path.join(folder, '*.txt'))\n",
    "\n",
    "# Initialize an empty string to hold the contents of all files\n",
    "all_text = ''\n",
    "\n",
    "# Loop through the list of files and read each one\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='latin-1') as f:\n",
    "        all_text += f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "wss_truc = [] # this is the list that will contain the text without the empty lines\n",
    "for line in all_text.split('\\n'):\n",
    "    if not re.match(r'^\\s*$', line):\n",
    "        # Split the line into sentences and add them to wss_truc\n",
    "        wss_truc.extend(nltk.tokenize.sent_tokenize(line))\n",
    "\n",
    "# Now wss_truc is a list of sentences from all_text but without the empty or whitespace-only lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 75.0\n",
      "Q3: 194.0\n",
      "IQR: 119.0\n",
      "Lower bound: -103.5\n",
      "Upper bound: 372.5\n",
      "Before:  5245\n",
      "After:  5014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the length of each sentence\n",
    "lengths = [len(s) for s in wss_truc]\n",
    "\n",
    "# Calculate the first and third quartiles\n",
    "Q1, Q3 = np.percentile(lengths, [25, 75])\n",
    "\n",
    "# Calculate the interquartile range\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "\n",
    "print('Q1:', Q1)\n",
    "print('Q3:', Q3)\n",
    "print('IQR:', IQR)\n",
    "print('Lower bound:', lower_bound)\n",
    "print('Upper bound:', upper_bound)\n",
    "print(\"Before: \",len(wss_truc))\n",
    "# Filter out sentences that are too short or too long\n",
    "wss_truc = [s for s in wss_truc if lower_bound <= len(s) <= upper_bound]\n",
    "print(\"After: \",len(wss_truc))\n",
    "\n",
    "\n",
    "# Now filtered_sentences contains the sentences from wss_truc without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Sentence (in terms of words):\n",
      "from this day forward, let each of us make a solemn commitment in his own heart: to bear his responsibility, to do his part, to live his ideals -- so that together, we can see the dawn of a new age of progress for america, and together, as we celebrate our 200th anniversary as a nation, we can do so proud in the fulfillment of our promise to ourselves and to the world.\n",
      "Number of Words: 82\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "longest_sentence = \"\"\n",
    "max_words = 0\n",
    "\n",
    "# Iterate over each sentence\n",
    "for sentence in wss_truc:\n",
    "    sentence = sentence.lower()\n",
    "    # Split the sentence into words\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Calculate the length of the sentence in terms of words\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Check if the current sentence is longer than the previous longest sentence\n",
    "    if num_words > max_words:\n",
    "        longest_sentence = sentence\n",
    "        max_words = num_words\n",
    "\n",
    "# Print the longest sentence\n",
    "print(\"Longest Sentence (in terms of words):\")\n",
    "print(longest_sentence)\n",
    "print(\"Number of Words:\", max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has given new inspiration to the power of self-help in both races by making labor more honorable to the one and more necessary to the other.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5014"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wss_truc[123])\n",
    "len(wss_truc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'so', 'shall', 'America', '--', 'in', 'the', 'sight', 'of', 'all', 'men', 'of', 'good', 'will', '--', 'prove', 'true', 'to', 'the', 'honorable', 'purposes', 'that', 'bind', 'and', 'rule', 'us', 'as', 'a', 'people', 'in', 'all', 'this', 'time', 'of', 'trial', 'through', 'which', 'we', 'pass', '.']\n",
      "[-0.28473315  0.44451743  0.10804686  0.16759656 -0.16301374 -0.69708246\n",
      "  0.64475054  1.37534    -0.38064182 -0.63269645 -0.07735743 -0.91599035\n",
      " -0.09009782  0.28997844  0.33454528 -0.55048513  0.54270643 -0.34319982\n",
      " -0.16251072 -1.1748464   0.376412    0.1347105   0.6570753  -0.41065648\n",
      " -0.00812421  0.16120565 -0.3598451  -0.09833887 -0.6427525   0.14379099\n",
      "  0.78957313 -0.02207439  0.3437416  -0.5434185  -0.19112729  0.5663944\n",
      "  0.21047543 -0.3409867  -0.10815226 -0.64986265  0.10797101 -0.49992904\n",
      " -0.50319135 -0.19659083  0.8656051  -0.2389644  -0.38818136 -0.13980746\n",
      "  0.18878916  0.12746745  0.13509993 -0.4933366  -0.25579247  0.00315018\n",
      " -0.20153457  0.2993274   0.26391265 -0.2065971  -0.49516395  0.04321143\n",
      "  0.13995986 -0.5596513   0.49418798  0.04722417 -0.65330213  0.8414361\n",
      " -0.11844084  0.71552306 -0.8380572   0.70683086 -0.4174295   0.2695461\n",
      "  0.98591626  0.04755499  0.401475    0.20431778 -0.03219248 -0.21366721\n",
      " -0.5353704  -0.15397717 -0.3483362  -0.38406014 -0.3177292   0.8407254\n",
      " -0.23948336 -0.32578537  0.35185474  0.19187361  0.4485424   0.3019855\n",
      "  0.5600163   0.47165126  0.3105703  -0.13525008  1.1346968   0.50666046\n",
      "  0.16593665 -0.5570104   0.1296445  -0.14044517]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Create a list of sentences\n",
    "sentences = wss_truc\n",
    "sentences = (([word_tokenize(sentence) for sentence in sentences]))\n",
    "print(sentences[9])\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=100)\n",
    "\n",
    "\n",
    "# Get the vector representation of a word\n",
    "vector = model.wv['freedom']\n",
    "\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Indices length:  5014 \n",
      "First sentence:  [26, 6471, 2, 1719]\n",
      "Sentence Tensors length:  5014 \n",
      "First sentence:  tensor([  26, 6471,    2, 1719])\n",
      "Padded sentences size:  torch.Size([5014, 82])\n",
      "Padded 999th sentence: \n",
      " tensor([ 145,   12,  925,    8,  337,    5, 3080,    4, 2551,    1,    0, 3068,\n",
      "          17,  961,  101,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "index to key: \n",
      " If\n",
      "[-0.11634699  0.2099309   0.06340787  0.05908985 -0.06434754 -0.42074615\n",
      "  0.32349724  0.7728614  -0.16076288 -0.360425   -0.080445   -0.5496361\n",
      " -0.04884283  0.18747592  0.16605605 -0.3207983   0.33070526 -0.21346548\n",
      " -0.08112041 -0.69344425  0.24633972  0.10603312  0.38477662 -0.27950922\n",
      "  0.01686421  0.10557933 -0.23904923 -0.05571788 -0.33645558  0.11262562\n",
      "  0.47024342 -0.01957674  0.20634761 -0.30184785 -0.12874264  0.34696126\n",
      "  0.09794179 -0.21136406 -0.06729104 -0.34021282  0.06897464 -0.2658892\n",
      " -0.22836111 -0.07160879  0.4202741  -0.16364832 -0.20945749 -0.12490483\n",
      "  0.07413489  0.08372609  0.14275011 -0.25541806 -0.09001695  0.01697093\n",
      " -0.14841086  0.19125633  0.16710204 -0.0683992  -0.19630256  0.0500721\n",
      "  0.09379944 -0.29872558  0.24851488  0.04480186 -0.3371219   0.4223906\n",
      " -0.09565748  0.30810305 -0.4058529   0.34367216 -0.24399418  0.13980453\n",
      "  0.49812177  0.06312238  0.27015945  0.08537152  0.02531412 -0.11591651\n",
      " -0.29970467 -0.03066164 -0.1978215  -0.13911763 -0.17892888  0.42262706\n",
      " -0.15852748 -0.14633267  0.15885094  0.06415592  0.23337539  0.15173015\n",
      "  0.36504892  0.2714021   0.14449069 -0.04466899  0.5873687   0.23979749\n",
      "  0.08103677 -0.25268918  0.02337065 -0.10042839]\n",
      "5014\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# Convert words to indices\n",
    "# simply a sentence made of indices of the words, not the words themselves\n",
    "# embedded words.\n",
    "sentences_indices = [[model.wv.key_to_index[word] for word in sentence] for sentence in sentences]\n",
    "print (\"Sentence Indices length: \", len(sentences_indices), \"\\nFirst sentence: \", sentences_indices[0])\n",
    "# Convert lists to tensors\n",
    "sentences_tensors = [torch.tensor(sentence) for sentence in sentences_indices]\n",
    "print (\"Sentence Tensors length: \", len(sentences_tensors), \"\\nFirst sentence: \", sentences_tensors[0])\n",
    "\n",
    "# Pad sequences\n",
    "padded_sentences = pad_sequence(sentences_tensors, batch_first=True, padding_value=0)\n",
    "print(\"Padded sentences size: \",padded_sentences.size())\n",
    "print(\"Padded 999th sentence: \\n\",padded_sentences[999])\n",
    "print (\"index to key: \\n\",model.wv.index_to_key[sentences_tensors[999][0]])\n",
    "print (model.wv.get_vector(model.wv.index_to_key[sentences_tensors[999][0]]))\n",
    "print (len(sentences_tensors)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itv(index):\n",
    "    return model.wv.get_vector(model.wv.index_to_key[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vti(vector):\n",
    "    return model.wv.key_to_index[model.wv.similar_by_vector(vector)[0][0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words processed:  129754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embdedded_sentences = torch.zeros((len(sentences_tensors), max_words, 100))\n",
    "total_words = 0\n",
    "for i in range(len(sentences_tensors)):\n",
    "    for j in range(len(sentences_tensors[i])):\n",
    "        vector = torch.tensor(itv(sentences_tensors[i][j]), dtype=torch.float)\n",
    "        for k in range(100):\n",
    "            embdedded_sentences[i][j][k] = vector[k]\n",
    "        total_words += 1\n",
    "print(\"Total words processed: \", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1163,  0.2099,  0.0634,  0.0591, -0.0643, -0.4207,  0.3235,  0.7729,\n",
       "        -0.1608, -0.3604, -0.0804, -0.5496, -0.0488,  0.1875,  0.1661, -0.3208,\n",
       "         0.3307, -0.2135, -0.0811, -0.6934,  0.2463,  0.1060,  0.3848, -0.2795,\n",
       "         0.0169,  0.1056, -0.2390, -0.0557, -0.3365,  0.1126,  0.4702, -0.0196,\n",
       "         0.2063, -0.3018, -0.1287,  0.3470,  0.0979, -0.2114, -0.0673, -0.3402,\n",
       "         0.0690, -0.2659, -0.2284, -0.0716,  0.4203, -0.1636, -0.2095, -0.1249,\n",
       "         0.0741,  0.0837,  0.1428, -0.2554, -0.0900,  0.0170, -0.1484,  0.1913,\n",
       "         0.1671, -0.0684, -0.1963,  0.0501,  0.0938, -0.2987,  0.2485,  0.0448,\n",
       "        -0.3371,  0.4224, -0.0957,  0.3081, -0.4059,  0.3437, -0.2440,  0.1398,\n",
       "         0.4981,  0.0631,  0.2702,  0.0854,  0.0253, -0.1159, -0.2997, -0.0307,\n",
       "        -0.1978, -0.1391, -0.1789,  0.4226, -0.1585, -0.1463,  0.1589,  0.0642,\n",
       "         0.2334,  0.1517,  0.3650,  0.2714,  0.1445, -0.0447,  0.5874,  0.2398,\n",
       "         0.0810, -0.2527,  0.0234, -0.1004])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embdedded_sentences[999][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_x, emb_y = [], []\n",
    "for sentence in embdedded_sentences:\n",
    "    for char_index in range(len(sentence)-1):\n",
    "        emb_x.append(sentence[char_index]) \n",
    "        emb_y.append(sentence[char_index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(emb_x))\n",
    "train_x, test_x = emb_x[:train_size], emb_x[train_size:]\n",
    "train_y, test_y = emb_y[:train_size], emb_y[train_size:]\n",
    "train_x = torch.stack(train_x)\n",
    "train_y = torch.stack(train_y)\n",
    "test_x = torch.stack(test_x)\n",
    "test_y = torch.stack(test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN architecture\n",
    "import torch.nn as nn\n",
    "rnn = nn.RNN(input_size=100, hidden_size=50, num_layers=3, batch_first=True)\n",
    "# Define dense layer\n",
    "dense_layer = nn.Linear(50, 9490)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(rnn.parameters()) + list(dense_layer.parameters()), lr=0.1)\n",
    "hidden = None\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9490\n"
     ]
    }
   ],
   "source": [
    "## get vocabulary size from sentences\n",
    "vocab_size = len(model.wv.key_to_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNNModel(nn.Module):\n",
    "#     def __init__(self, rnn_units, vocab_size):\n",
    "#         super(RNNModel, self).__init__()\n",
    "#         self.rnn = nn.RNN(input_size=rnn_units, hidden_size=rnn_units, batch_first=True)\n",
    "#         self.fc = nn.Linear(rnn_units, vocab_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output, hidden = self.rnn(x)\n",
    "#         output = self.fc(output[:, -1, :])  # Use the last timestep\n",
    "#         return output\n",
    "    \n",
    "# rnn = RNNModel(vocab_size=9490, rnn_units=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 9490])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[100, 9490]' is invalid for input of size 94900",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     31\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m sequence_length)\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, batch_y)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[100, 9490]' is invalid for input of size 94900"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume rnn, optimizer, criterion, train_x, train_y have been defined\n",
    "\n",
    "batch_size = 10\n",
    "iters = 300\n",
    "hidden = None  # Initial hidden state\n",
    "\n",
    "for epoch in range(iters):\n",
    "    batch_indices = np.random.choice(len(train_x), batch_size, replace=False)\n",
    "    batch_x = train_x[batch_indices]\n",
    "    batch_y = train_y[batch_indices]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output, hidden = rnn(batch_x,hidden)  # output shape: (batch_size, sequence_length, hidden_size)\n",
    "    output = dense_layer(output)  # output shape now: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "    # Reshape for loss calculation\n",
    "    print(output.shape)\n",
    "\n",
    "    output = output.reshape(-1, vocab_size)  # Flatten output: (batch_size * sequence_length, vocab_size)\n",
    "    batch_y = batch_y.view(-1)  # Flatten targets to match output: (batch_size * sequence_length)\n",
    "\n",
    "    # Compute loss, backpropagate, and update weights\n",
    "    # Assume sequence_length is defined\n",
    "    sequence_length = batch_x.shape[1]\n",
    "    output = output.reshape(batch_size * sequence_length, vocab_size)\n",
    "    batch_y = batch_y.view(batch_size * sequence_length)\n",
    "    loss = criterion(output, batch_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:', epoch, 'Loss:', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
