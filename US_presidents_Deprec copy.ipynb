{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "# Read the CSV file\n",
    "folder = 'inaugural/'\n",
    "# Get a list of all text files in the folder\n",
    "text_files = glob.glob(os.path.join(folder, '*.txt'))\n",
    "\n",
    "# Initialize an empty string to hold the contents of all files\n",
    "all_text = ''\n",
    "\n",
    "# Loop through the list of files and read each one\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='latin-1') as f:\n",
    "        all_text += f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "wss_truc = [] # this is the list that will contain the text without the empty lines\n",
    "for line in all_text.split('\\n'):\n",
    "    if not re.match(r'^\\s*$', line):\n",
    "        # Split the line into sentences and add them to wss_truc\n",
    "        wss_truc.extend(nltk.tokenize.sent_tokenize(line))\n",
    "\n",
    "# Now wss_truc is a list of sentences from all_text but without the empty or whitespace-only lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 75.0\n",
      "Q3: 194.0\n",
      "IQR: 119.0\n",
      "Lower bound: -103.5\n",
      "Upper bound: 372.5\n",
      "Before:  5245\n",
      "After:  5014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the length of each sentence\n",
    "lengths = [len(s) for s in wss_truc]\n",
    "\n",
    "# Calculate the first and third quartiles\n",
    "Q1, Q3 = np.percentile(lengths, [25, 75])\n",
    "\n",
    "# Calculate the interquartile range\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "\n",
    "print('Q1:', Q1)\n",
    "print('Q3:', Q3)\n",
    "print('IQR:', IQR)\n",
    "print('Lower bound:', lower_bound)\n",
    "print('Upper bound:', upper_bound)\n",
    "print(\"Before: \",len(wss_truc))\n",
    "# Filter out sentences that are too short or too long\n",
    "wss_truc = [s for s in wss_truc if lower_bound <= len(s) <= upper_bound]\n",
    "print(\"After: \",len(wss_truc))\n",
    "\n",
    "\n",
    "# Now filtered_sentences contains the sentences from wss_truc without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Sentence (in terms of words):\n",
      "from this day forward, let each of us make a solemn commitment in his own heart: to bear his responsibility, to do his part, to live his ideals -- so that together, we can see the dawn of a new age of progress for america, and together, as we celebrate our 200th anniversary as a nation, we can do so proud in the fulfillment of our promise to ourselves and to the world.\n",
      "Number of Words: 82\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "longest_sentence = \"\"\n",
    "max_words = 0\n",
    "\n",
    "# Iterate over each sentence\n",
    "for sentence in wss_truc:\n",
    "    sentence = sentence.lower()\n",
    "    # Split the sentence into words\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Calculate the length of the sentence in terms of words\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Check if the current sentence is longer than the previous longest sentence\n",
    "    if num_words > max_words:\n",
    "        longest_sentence = sentence\n",
    "        max_words = num_words\n",
    "\n",
    "# Print the longest sentence\n",
    "print(\"Longest Sentence (in terms of words):\")\n",
    "print(longest_sentence)\n",
    "print(\"Number of Words:\", max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has given new inspiration to the power of self-help in both races by making labor more honorable to the one and more necessary to the other.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5014"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wss_truc[123])\n",
    "len(wss_truc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'so', 'shall', 'America', '--', 'in', 'the', 'sight', 'of', 'all', 'men', 'of', 'good', 'will', '--', 'prove', 'true', 'to', 'the', 'honorable', 'purposes', 'that', 'bind', 'and', 'rule', 'us', 'as', 'a', 'people', 'in', 'all', 'this', 'time', 'of', 'trial', 'through', 'which', 'we', 'pass', '.']\n",
      "[-0.28405246  0.4119024   0.10824949  0.16703472 -0.13946846 -0.66802764\n",
      "  0.6123879   1.2793777  -0.3515644  -0.5937289  -0.07727722 -0.8684774\n",
      " -0.08548225  0.29978842  0.32530463 -0.5317908   0.54499584 -0.3512206\n",
      " -0.16638929 -1.185823    0.38119495  0.14687824  0.6630655  -0.41976824\n",
      "  0.01379933  0.15328032 -0.34413353 -0.09186853 -0.6370527   0.15738194\n",
      "  0.7906737  -0.0321282   0.30916205 -0.51986784 -0.18198511  0.55623424\n",
      "  0.18361825 -0.32970887 -0.08519449 -0.6310024   0.08940866 -0.49410096\n",
      " -0.5017738  -0.1898621   0.83101463 -0.23273668 -0.37591234 -0.12444883\n",
      "  0.19622968  0.12744991  0.15044947 -0.47755298 -0.23138498 -0.00989754\n",
      " -0.21064231  0.29296628  0.2589443  -0.20020941 -0.4665376   0.05946823\n",
      "  0.1216585  -0.53593916  0.48755905  0.05013034 -0.62151563  0.7956554\n",
      " -0.107577    0.6575616  -0.75480574  0.6530684  -0.39711595  0.27321064\n",
      "  0.93741065  0.04988211  0.36787164  0.21377644 -0.0251536  -0.21532649\n",
      " -0.50817686 -0.17397746 -0.3503779  -0.3714268  -0.31106132  0.799759\n",
      " -0.23689006 -0.3150227   0.34299642  0.15939076  0.42830378  0.29547226\n",
      "  0.53051555  0.44495124  0.28145227 -0.13049749  1.0874245   0.47551447\n",
      "  0.16072787 -0.5506184   0.13890216 -0.13085358]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Create a list of sentences\n",
    "sentences = wss_truc\n",
    "sentences = (([word_tokenize(sentence) for sentence in sentences]))\n",
    "print(sentences[9])\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=100)\n",
    "\n",
    "\n",
    "# Get the vector representation of a word\n",
    "vector = model.wv['freedom']\n",
    "\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Indices length:  5014 \n",
      "First sentence:  [26, 6471, 2, 1719]\n",
      "Sentence Tensors length:  5014 \n",
      "First sentence:  tensor([  26, 6471,    2, 1719])\n",
      "Padded sentences size:  torch.Size([5014, 82])\n",
      "Padded 999th sentence: \n",
      " tensor([ 145,   12,  925,    8,  337,    5, 3080,    4, 2551,    1,    0, 3068,\n",
      "          17,  961,  101,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "index to key: \n",
      " If\n",
      "[-0.14499563  0.22462133  0.0657555   0.07042288 -0.0598119  -0.44081974\n",
      "  0.3427372   0.7950321  -0.17258732 -0.3573246  -0.08286595 -0.5512895\n",
      " -0.04575876  0.20186509  0.17450239 -0.32679158  0.31442824 -0.2106981\n",
      " -0.08093372 -0.661403    0.22843152  0.09455459  0.3648432  -0.26457846\n",
      "  0.02381971  0.09237166 -0.21461956 -0.05372439 -0.32039598  0.11425555\n",
      "  0.4503853  -0.02082068  0.19543739 -0.32385576 -0.12731759  0.35766524\n",
      "  0.09341234 -0.22267346 -0.05887035 -0.36936173  0.05997487 -0.28401253\n",
      " -0.25730765 -0.08134375  0.45444897 -0.17089437 -0.22125052 -0.11296976\n",
      "  0.09128313  0.08026341  0.1426283  -0.2571484  -0.09512902  0.00525348\n",
      " -0.15079327  0.18054375  0.17552868 -0.08285286 -0.20581615  0.04980963\n",
      "  0.08894774 -0.3008769   0.2598007   0.04571855 -0.3628082   0.45231977\n",
      " -0.08872865  0.33017457 -0.4168067   0.35571334 -0.25220236  0.15409948\n",
      "  0.5053994   0.0606483   0.25611457  0.10025246  0.02025244 -0.11933851\n",
      " -0.29451728 -0.05099193 -0.20223588 -0.15084238 -0.1766894   0.41478464\n",
      " -0.14500937 -0.1484025   0.15653603  0.05374436  0.24801157  0.15968296\n",
      "  0.35679337  0.27152792  0.13553086 -0.05438723  0.5974759   0.24051598\n",
      "  0.08631258 -0.2808258   0.0465209  -0.09680542]\n",
      "5014\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# Convert words to indices\n",
    "# simply a sentence made of indices of the words, not the words themselves\n",
    "# embedded words.\n",
    "sentences_indices = [[model.wv.key_to_index[word] for word in sentence] for sentence in sentences]\n",
    "print (\"Sentence Indices length: \", len(sentences_indices), \"\\nFirst sentence: \", sentences_indices[0])\n",
    "# Convert lists to tensors\n",
    "sentences_tensors = [torch.tensor(sentence) for sentence in sentences_indices]\n",
    "print (\"Sentence Tensors length: \", len(sentences_tensors), \"\\nFirst sentence: \", sentences_tensors[0])\n",
    "\n",
    "# Pad sequences\n",
    "padded_sentences = pad_sequence(sentences_tensors, batch_first=True, padding_value=0)\n",
    "print(\"Padded sentences size: \",padded_sentences.size())\n",
    "print(\"Padded 999th sentence: \\n\",padded_sentences[999])\n",
    "print (\"index to key: \\n\",model.wv.index_to_key[sentences_tensors[999][0]])\n",
    "print (model.wv.get_vector(model.wv.index_to_key[sentences_tensors[999][0]]))\n",
    "print (len(sentences_tensors)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itv(index):\n",
    "    return model.wv.get_vector(model.wv.index_to_key[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vti(vector):\n",
    "    return model.wv.key_to_index[model.wv.similar_by_vector(vector)[0][0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  129754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embdedded_sentences = torch.zeros((len(sentences_tensors), max_words, 100))\n",
    "total_words = 0\n",
    "for i in range(len(sentences_tensors)):\n",
    "    for j in range(len(sentences_tensors[i])):\n",
    "        vector = torch.tensor(itv(sentences_tensors[i][j]), dtype=torch.float)\n",
    "        for k in range(100):\n",
    "            embdedded_sentences[i][j][k] = vector[k]\n",
    "        total_words += 1\n",
    "print(\"Total words processed: \", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1219,  0.1976,  0.0597,  0.0494, -0.0550, -0.4029,  0.3109,  0.7377,\n",
       "        -0.1582, -0.3444, -0.0861, -0.5294, -0.0469,  0.1828,  0.1634, -0.3110,\n",
       "         0.3070, -0.1975, -0.0727, -0.6460,  0.2329,  0.0934,  0.3645, -0.2665,\n",
       "         0.0218,  0.0956, -0.2143, -0.0535, -0.3105,  0.1085,  0.4380, -0.0138,\n",
       "         0.1978, -0.3154, -0.1243,  0.3496,  0.0967, -0.2195, -0.0612, -0.3483,\n",
       "         0.0698, -0.2885, -0.2461, -0.0734,  0.4502, -0.1692, -0.2203, -0.1232,\n",
       "         0.0825,  0.0777,  0.1409, -0.2501, -0.0866,  0.0113, -0.1511,  0.1809,\n",
       "         0.1712, -0.0750, -0.1969,  0.0489,  0.0992, -0.3028,  0.2615,  0.0447,\n",
       "        -0.3551,  0.4530, -0.0941,  0.3319, -0.4229,  0.3616, -0.2558,  0.1492,\n",
       "         0.5296,  0.0683,  0.2779,  0.1013,  0.0280, -0.1226, -0.3087, -0.0450,\n",
       "        -0.2077, -0.1520, -0.1887,  0.4424, -0.1546, -0.1546,  0.1604,  0.0649,\n",
       "         0.2432,  0.1539,  0.3663,  0.2740,  0.1383, -0.0531,  0.5977,  0.2444,\n",
       "         0.0865, -0.2631,  0.0374, -0.1046])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embdedded_sentences[999][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_x, emb_y = [], []\n",
    "for sentence in embdedded_sentences:\n",
    "    for char_index in range(len(sentence)-1):\n",
    "        emb_x.append(sentence[char_index]) \n",
    "        emb_y.append(sentence[char_index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(emb_x))\n",
    "train_x, test_x = emb_x[:train_size], emb_x[train_size:]\n",
    "train_y, test_y = emb_y[:train_size], emb_y[train_size:]\n",
    "train_x = torch.stack(train_x)\n",
    "train_y = torch.stack(train_y)\n",
    "test_x = torch.stack(test_x)\n",
    "test_y = torch.stack(test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN architecture\n",
    "import torch.nn as nn\n",
    "rnn = nn.RNN(input_size=100, hidden_size=50, num_layers=3, batch_first=True)\n",
    "# Define dense layer\n",
    "dense_layer = nn.Linear(50, 9490)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(rnn.parameters()) + list(dense_layer.parameters()), lr=0.1)\n",
    "hidden = None\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9490\n"
     ]
    }
   ],
   "source": [
    "## get vocabulary size from sentences\n",
    "vocab_size = len(model.wv.key_to_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNNModel(nn.Module):\n",
    "#     def __init__(self, rnn_units, vocab_size):\n",
    "#         super(RNNModel, self).__init__()\n",
    "#         self.rnn = nn.RNN(input_size=rnn_units, hidden_size=rnn_units, batch_first=True)\n",
    "#         self.fc = nn.Linear(rnn_units, vocab_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output, hidden = self.rnn(x)\n",
    "#         output = self.fc(output[:, -1, :])  # Use the last timestep\n",
    "#         return output\n",
    "    \n",
    "# rnn = RNNModel(vocab_size=9490, rnn_units=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 9490])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[94900, 9490]' is invalid for input of size 94900",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Compute loss, backpropagate, and update weights\u001b[39;00m\n\u001b[1;32m     29\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Correctly calculate sequence_length\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m sequence_length)\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, batch_y)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[94900, 9490]' is invalid for input of size 94900"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume rnn, optimizer, criterion, train_x, train_y have been defined\n",
    "\n",
    "batch_size = 10\n",
    "iters = 300\n",
    "hidden = None  # Initial hidden state\n",
    "\n",
    "for epoch in range(iters):\n",
    "    batch_indices = np.random.choice(len(train_x), batch_size, replace=False)\n",
    "    batch_x = train_x[batch_indices]\n",
    "    batch_y = train_y[batch_indices]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output, hidden = rnn(batch_x,hidden)  # output shape: (batch_size, sequence_length, hidden_size)\n",
    "    output = dense_layer(output)  # output shape now: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "    # Reshape for loss calculation\n",
    "    print(output.shape)\n",
    "\n",
    "    output = output.reshape(-1, vocab_size)  # Flatten output: (batch_size * sequence_length, vocab_size)\n",
    "    batch_y = batch_y.view(-1)  # Flatten targets to match output: (batch_size * sequence_length)\n",
    "\n",
    "    # Compute loss, backpropagate, and update weights\n",
    "    # Assume sequence_length is defined\n",
    "    sequence_length = batch_x.shape[1]\n",
    "    output = output.reshape(batch_size * sequence_length, vocab_size)\n",
    "    batch_y = batch_y.view(batch_size * sequence_length)\n",
    "    loss = criterion(output, batch_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:', epoch, 'Loss:', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
