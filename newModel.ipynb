{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "# Read the CSV file\n",
    "folder = 'inaugural/'\n",
    "# Get a list of all text files in the folder\n",
    "text_files = glob.glob(os.path.join(folder, '*.txt'))\n",
    "\n",
    "# Initialize an empty string to hold the contents of all files\n",
    "all_text = ''\n",
    "\n",
    "# Loop through the list of files and read each one\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='latin-1') as f:\n",
    "        all_text += f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "wss_truc = [] # this is the list that will contain the text without the empty lines\n",
    "for line in all_text.split('\\n'):\n",
    "    if not re.match(r'^\\s*$', line):\n",
    "        # Split the line into sentences and add them to wss_truc\n",
    "        wss_truc.extend(nltk.tokenize.sent_tokenize(line))\n",
    "\n",
    "# Now wss_truc is a list of sentences from all_text but without the empty or whitespace-only lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 75.0\n",
      "Q3: 194.0\n",
      "IQR: 119.0\n",
      "Lower bound: -103.5\n",
      "Upper bound: 372.5\n",
      "Before:  5245\n",
      "After:  5014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the length of each sentence\n",
    "lengths = [len(s) for s in wss_truc]\n",
    "\n",
    "# Calculate the first and third quartiles\n",
    "Q1, Q3 = np.percentile(lengths, [25, 75])\n",
    "\n",
    "# Calculate the interquartile range\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "\n",
    "print('Q1:', Q1)\n",
    "print('Q3:', Q3)\n",
    "print('IQR:', IQR)\n",
    "print('Lower bound:', lower_bound)\n",
    "print('Upper bound:', upper_bound)\n",
    "print(\"Before: \",len(wss_truc))\n",
    "# Filter out sentences that are too short or too long\n",
    "wss_truc = [s for s in wss_truc if lower_bound <= len(s) <= upper_bound]\n",
    "print(\"After: \",len(wss_truc))\n",
    "\n",
    "\n",
    "# Now filtered_sentences contains the sentences from wss_truc without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Sentence (in terms of words):\n",
      "From this day forward, let each of us make a solemn commitment in his own heart: to bear his responsibility, to do his part, to live his ideals -- so that together, we can see the dawn of a new age of progress for America, and together, as we celebrate our 200th anniversary as a nation, we can do so proud in the fulfillment of our promise to ourselves and to the world.\n",
      "Number of Words: 82\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "longest_sentence = \"\"\n",
    "max_words = 0\n",
    "\n",
    "# Iterate over each sentence\n",
    "for sentence in wss_truc:\n",
    "    # Split the sentence into words\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Calculate the length of the sentence in terms of words\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Check if the current sentence is longer than the previous longest sentence\n",
    "    if num_words > max_words:\n",
    "        longest_sentence = sentence\n",
    "        max_words = num_words\n",
    "\n",
    "# Print the longest sentence\n",
    "print(\"Longest Sentence (in terms of words):\")\n",
    "print(longest_sentence)\n",
    "print(\"Number of Words:\", max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has given new inspiration to the power of self-help in both races by making labor more honorable to the one and more necessary to the other.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5014"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wss_truc[123])\n",
    "len(wss_truc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Price', 'of', 'Peace']\n",
      "[-0.2525971   0.42700943  0.12013165  0.12152048 -0.21124654 -0.63928825\n",
      "  0.58684766  1.2170982  -0.3955623  -0.6425838  -0.03964337 -0.8533821\n",
      " -0.05216161  0.24543197  0.29338062 -0.5560536   0.54698694 -0.35781538\n",
      " -0.16653399 -1.1446062   0.380421    0.17019045  0.6375555  -0.4100209\n",
      " -0.04102421  0.17423284 -0.3612731  -0.09456202 -0.6529709   0.14056787\n",
      "  0.74061733 -0.03772156  0.3182855  -0.52975136 -0.170171    0.51363254\n",
      "  0.1628626  -0.28580457 -0.12979947 -0.6265941   0.10117768 -0.5048628\n",
      " -0.47761884 -0.1641562   0.75065684 -0.22618842 -0.3606702  -0.13749631\n",
      "  0.19925246  0.15306814  0.10901764 -0.46054494 -0.23847114  0.00395064\n",
      " -0.20914641  0.34535694  0.23675218 -0.16146532 -0.45600274  0.08340442\n",
      "  0.12827031 -0.5198007   0.40631774  0.04078281 -0.5866992   0.76675445\n",
      " -0.1001404   0.62999004 -0.7631723   0.6220139  -0.38637537  0.23489694\n",
      "  0.9435234   0.03456828  0.36894706  0.20237617 -0.02883388 -0.19564489\n",
      " -0.50460345 -0.14156973 -0.31551418 -0.3251763  -0.30134195  0.7493202\n",
      " -0.214832   -0.33309758  0.3060549   0.20395392  0.42525247  0.2759613\n",
      "  0.531623    0.40059444  0.23254701 -0.14153136  1.0104533   0.45754325\n",
      "  0.19109744 -0.5280709   0.09721576 -0.14419705]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Create a list of sentences\n",
    "sentences = wss_truc\n",
    "sentences = (([word_tokenize(sentence) for sentence in sentences]))\n",
    "print(sentences[0])\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=100)\n",
    "\n",
    "\n",
    "# Get the vector representation of a word\n",
    "vector = model.wv['freedom']\n",
    "\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 145,   12,  925,    8,  337,    5, 3080,    4, 2551,    1,    0, 3068,\n",
      "          17,  961,  101,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "If\n",
      "[-0.12183109  0.2202061   0.06920862  0.03178521 -0.10502056 -0.4375917\n",
      "  0.33363536  0.7743538  -0.18722387 -0.3823133  -0.07506983 -0.5408861\n",
      " -0.03004371  0.1675987   0.1504226  -0.3310754   0.3228592  -0.21258025\n",
      " -0.08066671 -0.6511852   0.2444657   0.1138318   0.36982757 -0.27240878\n",
      " -0.00189271  0.1106787  -0.22765437 -0.0620686  -0.32771394  0.10808066\n",
      "  0.42848128 -0.02413391  0.20846245 -0.34507936 -0.12417553  0.3508459\n",
      "  0.08127075 -0.20507336 -0.08943505 -0.37628826  0.07229684 -0.3081904\n",
      " -0.2519407  -0.06821269  0.42353272 -0.1724207  -0.22570236 -0.13481137\n",
      "  0.08724533  0.09012742  0.12339238 -0.23666567 -0.08983318  0.01450136\n",
      " -0.15825954  0.21452938  0.16565901 -0.05712184 -0.19912656  0.06727472\n",
      "  0.1022838  -0.30203676  0.21993661  0.04523041 -0.34642345  0.44948214\n",
      " -0.09178212  0.3166417  -0.42214394  0.3460168  -0.25770488  0.13866217\n",
      "  0.5176657   0.06200719  0.2688184   0.09722198  0.0263403  -0.11483981\n",
      " -0.29833436 -0.02784231 -0.18726552 -0.13028547 -0.17980856  0.39408186\n",
      " -0.14054154 -0.16236539  0.1393714   0.07520029  0.23437288  0.14322193\n",
      "  0.36640728  0.24886404  0.10862368 -0.06078691  0.5505307   0.22630696\n",
      "  0.10580037 -0.26441056  0.02266037 -0.11023644]\n",
      "5014\n"
     ]
    }
   ],
   "source": [
    "# Convert words to indices\n",
    "sentences_indices = [[model.wv.key_to_index[word] for word in sentence] for sentence in sentences]\n",
    "\n",
    "# Convert lists to tensors\n",
    "sentences_tensors = [torch.tensor(sentence) for sentence in sentences_indices]\n",
    "\n",
    "# Pad sequences\n",
    "padded_sentences = pad_sequence(sentences_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "print(padded_sentences[999])\n",
    "print (model.wv.index_to_key[sentences_tensors[999][0]])\n",
    "print (model.wv.get_vector(model.wv.index_to_key[sentences_tensors[999][0]]))\n",
    "print (len(sentences_tensors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itv(index):\n",
    "    return model.wv.get_vector(model.wv.index_to_key[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embdedded_sentences = torch.zeros((len(sentences_tensors), max_words, 100))\n",
    "for i in range(len(sentences_tensors)):\n",
    "    for j in range(len(sentences_tensors[i])):\n",
    "        vector = torch.tensor(itv(sentences_tensors[i][j]), dtype=torch.float)\n",
    "        for k in range(100):\n",
    "            embdedded_sentences[i][j][k] = vector[k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1218,  0.2202,  0.0692,  ..., -0.2644,  0.0227, -0.1102],\n",
       "        [-0.3499,  0.4991,  0.2468,  ..., -0.6421,  0.0628, -0.1999],\n",
       "        [-0.0396,  0.0790,  0.0319,  ..., -0.0759,  0.0069, -0.0372],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embdedded_sentences[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_x, emb_y = [], []\n",
    "for sentence in embdedded_sentences:\n",
    "    for char_index in range(len(sentence)-1):\n",
    "        emb_x.append(sentence[char_index]) \n",
    "        emb_y.append(sentence[char_index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(emb_x))\n",
    "train_x, test_x = emb_x[:train_size], emb_x[train_size:]\n",
    "train_y, test_y = emb_y[:train_size], emb_y[train_size:]\n",
    "train_x = torch.stack(train_x)\n",
    "train_y = torch.stack(train_y)\n",
    "test_x = torch.stack(test_x)\n",
    "test_y = torch.stack(test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN architecture\n",
    "rnn = nn.RNN(input_size=100, hidden_size=50, num_layers=3, batch_first=True)\n",
    "# Define dense layer\n",
    "dense_layer = nn.Linear(50, 9490)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(rnn.parameters()) + list(dense_layer.parameters()), lr=0.1)\n",
    "hidden = None\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9490\n"
     ]
    }
   ],
   "source": [
    "## get vocabulary size from sentences\n",
    "vocab_size = len(model.wv.key_to_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNNModel(nn.Module):\n",
    "#     def __init__(self, rnn_units, vocab_size):\n",
    "#         super(RNNModel, self).__init__()\n",
    "#         self.rnn = nn.RNN(input_size=rnn_units, hidden_size=rnn_units, batch_first=True)\n",
    "#         self.fc = nn.Linear(rnn_units, vocab_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output, hidden = self.rnn(x)\n",
    "#         output = self.fc(output[:, -1, :])  # Use the last timestep\n",
    "#         return output\n",
    "    \n",
    "# rnn = RNNModel(vocab_size=9490, rnn_units=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32) to match target batch_size (3200).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[281], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m batch_y_flat \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (3200)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume rnn, optimizer, criterion, train_x, train_y have been defined\n",
    "\n",
    "batch_size = 32\n",
    "iters = 300\n",
    "hidden = None  # Initial hidden state\n",
    "\n",
    "for epoch in range(iters):\n",
    "    batch_indices = np.random.choice(len(train_x), batch_size, replace=False)\n",
    "    batch_x = train_x[batch_indices]\n",
    "    batch_y = train_y[batch_indices]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output, hidden = rnn(batch_x,hidden)  # output shape: (batch_size, sequence_length, hidden_size)\n",
    "    output = dense_layer(output)  # output shape now: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "    # Reshape for loss calculation\n",
    "    print(output.shape)\n",
    "\n",
    "    output = output.reshape(-1, vocab_size)  # Flatten output: (batch_size * sequence_length, vocab_size)\n",
    "    batch_y = batch_y.view(-1)  # Flatten targets to match output: (batch_size * sequence_length)\n",
    "\n",
    "    # Compute loss, backpropagate, and update weights\n",
    "    loss = criterion(output, batch_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:', epoch, 'Loss:', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
